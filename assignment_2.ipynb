{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a10a86-67e7-455f-99bb-1f089085a9d4",
   "metadata": {},
   "source": [
    "# Task 1 - NaÃ¯ve Bayes Classifier\n",
    "\n",
    "Install requirements:\n",
    "\n",
    "- pandas\n",
    "- nltk\n",
    "- sklearn\n",
    "\n",
    "In the cell below is all relevant code for part 1, the run_ml_pipeline function runs the entire pipeline and outputs performance metrics for the Naive Bayes classifier, which will be called further down the notebook.\n",
    "\n",
    "Please note the code comment in run_ml_pipeline discusses how the pipeline handles the presense of words unseen in the training set both as a whole or of a specific class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bc7bca9-fd38-4f64-9d0d-7a7e32fdb23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Jack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Jack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import downloader\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "downloader.download(\"stopwords\")\n",
    "downloader.download(\"punkt_tab\")\n",
    "downloader.download(\"wordnet\")\n",
    "downloader.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "english_stop_words = set(stopwords.words(\"english\"))\n",
    "all_stop_words = set()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# add versions of a stopword with and without apostrophes\n",
    "# e.g. \"don't\" and \"dont\"\n",
    "for word in english_stop_words:\n",
    "    if \"'\" in word:\n",
    "        all_stop_words.add(word.replace(\"'\", \"\"))\n",
    "    else:\n",
    "        all_stop_words.add(word)\n",
    "\n",
    "input_data = pd.read_csv(\"car-reviews.csv\")\n",
    "\n",
    "def preprocessor(text: str) -> str:\n",
    "    \"\"\"Preprocesses text by removing punctuation and stopwords, stemming, and lowercasing.\"\"\"\n",
    "\n",
    "    text = preprocess_remove_punctuation(text)\n",
    "    text = preprocess_lowercase(text)\n",
    "    text = preprocess_remove_stopwords(text)\n",
    "    text = preprocess_stem(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_lowercase(text: str) -> str:\n",
    "    \"\"\"Preprocesses text by lowercasing.\"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def preprocess_remove_punctuation(text: str) -> str:\n",
    "    \"\"\"Preprocesses text by removing punctuation \"\"\"\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def preprocess_remove_stopwords(text: str) -> str:\n",
    "    \"\"\"Preprocesses text by removing stopwords.\"\"\"\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in all_stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def preprocess_stem(text: str) -> str:\n",
    "    \"\"\"Preprocesses text by stemming.\"\"\"\n",
    "\n",
    "    def pos_to_wordnet(pos: str) -> str:\n",
    "        \"\"\"Converts POS tag to WordNet format.\"\"\"\n",
    "\n",
    "        first_char = pos[0]\n",
    "\n",
    "        if first_char == \"J\":\n",
    "            return \"a\"\n",
    "        elif first_char == \"V\":\n",
    "            return \"v\"\n",
    "        elif first_char == \"N\":\n",
    "            return \"n\"\n",
    "        elif first_char == \"R\":\n",
    "            return \"r\"\n",
    "        else:\n",
    "            return \"n\"\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    token_tags = pos_tag(tokens)\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(word, pos_to_wordnet(pos)) for word, pos in token_tags]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "class Preprocessor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [preprocessor(text) for text in X]\n",
    "\n",
    "\n",
    "def run_ml_pipeline(input_data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Runs the machine learning pipeline on input dataframe.\n",
    "    \"\"\"\n",
    "    test_size = 276  # 20% of the data for testing\n",
    "\n",
    "    training_data, test_data = train_test_split(\n",
    "        input_data,\n",
    "        test_size=test_size,\n",
    "        random_state=120,\n",
    "        stratify=input_data[\"Sentiment\"],\n",
    "    )\n",
    "\n",
    "    # ----- A note about unseen words in the test set ------:\n",
    "    # The MultinomialNB Classifier is passed an alpha value of 1.0, which happens to be the default value.\n",
    "    # This is the additive laplace smoothing parameter and means that all features will have a constant value of 1.0 added to them.\n",
    "    # This will essentially mean when an unseen word is encountered in the test set, it will be treated as if it has been seen once in the training set.\n",
    "    # This is much better than the unseen word having a probability of 0.0 which would cause Naive Bayes to be unable to classify the review.\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"preprocess\", Preprocessor()),\n",
    "            (\"vectorize\", CountVectorizer()),\n",
    "            (\"classify\", MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline.fit(training_data[\"Review\"], training_data[\"Sentiment\"])\n",
    "\n",
    "    predictions = pipeline.predict(test_data[\"Review\"])\n",
    "\n",
    "    accuracy = accuracy_score(test_data[\"Sentiment\"], predictions)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    labels = [\"Pos\", \"Neg\"]\n",
    "\n",
    "    cm = confusion_matrix(test_data[\"Sentiment\"], predictions, labels=labels)\n",
    "\n",
    "    cm_normalized = cm.astype(\"float\") / cm.sum()\n",
    "\n",
    "    print(\"Confusion Matrix Proportions:\")\n",
    "    print(f\"True Negative: {cm_normalized[0][0] * 100:.2f}%\")\n",
    "    print(f\"False Negative: {cm_normalized[1][0] * 100:.2f}%\")\n",
    "    print(f\"True Positive: {cm_normalized[1][1] * 100:.2f}%\")\n",
    "    print(f\"False Positive: {cm_normalized[0][1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e775a8-bd9b-450f-b999-0d8b84621b79",
   "metadata": {},
   "source": [
    "The cell below demonstrates that the ML pipeline implemented preprocesses the reviews by removing stop-words and punctuation.\n",
    "\n",
    "The preprocesser function is called which is the first step as part of the sklearn Pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "385dbcd2-98f3-449c-a3a6-0abc4d42bd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "  In 1992 we bought a new Taurus and we really loved it  So in 1999 we decided to try a new Taurus  I did not care for the style of the newer version  but bought it anyway I do not like the new car half as much as i liked our other one  Thee dash is much to deep and takes up a lot of room  I do not find the seats as comfortable and the way the sides stick out further than the strip that should protect your card from denting It drives nice and has good pick up  But you can not see the hood at all from the driver seat and judging and parking is difficult  It has a very small gas tank I would not buy a Taurus if I had it to do over  I would rather have my 1992 back  I dont think the style is as nice as the the 1992  and it was a mistake to change the style  In less than a month we had a dead battery and a flat tire  \n",
      "\n",
      "Processed: \n",
      " 1992 bought new taurus really loved 1999 decided try new taurus care style newer version bought anyway like new car half much liked one thee dash much deep takes lot room find seats comfortable way sides stick strip protect card denting drives nice good pick see hood driver seat judging parking difficult small gas tank would buy taurus would rather 1992 back think style nice 1992 mistake change style less month dead battery flat tire \n",
      "\n",
      "----------------------\n",
      "\n",
      "Original: \n",
      "  The last business trip  I drove to San Francisco  I went to Hertz Rentals and got a 1999 Ford Taurus  thinking it looked comfortable and professional  I found the seating to be uncomfortable for myself  as well as for my passenger Now  seating comfort may not be important to you  but it is to me The fuel usage was fine  the car did get us there with no problems  but  it was such an uncomfortable ride for both of us  It is not as though I am hard to fit into a car  I am 5 5  weigh 115 pounds  and I am usually quite comfortable in most any car  But  the Taurus seemed especially uncomfortable  For anyone who is thinking of renting a car for a long trip  I would suggest that the Ford Taurus  1999  not be on the list of  be sure to get this There was a large trunk that allowed us to pack a lot of materials needed for the business end of the trip  Driving it was a breeze  and everything  worked  right  I have no complaints about the standard making of the car  except in the comfort area  How important is comfort to you  On a long trip  being an uncomfortable driver is distracting to me  and it made the trip seem unnecessarily longer than it should have felt  I make this trip a lot  The Saturn would have been a better choice for me  as I have rented this model quite often  It offers comforts that the Taurus simply does not have If you are planning a long trip  or to buy a car that you intend to be comfortably seated in  check out the Taurus on a longer trial basis than you might another car  You may find the same thing happening to your comfort   \n",
      "\n",
      "Processed: \n",
      " last business trip drove san francisco went hertz rentals got 1999 ford taurus thinking looked comfortable professional found seating uncomfortable passenger seating comfort may important fuel usage fine car get us problems uncomfortable ride us though hard fit car 5 5 weigh 115 pounds usually quite comfortable car taurus seemed especially uncomfortable anyone thinking renting car long trip would suggest ford taurus 1999 list sure get large trunk allowed us pack lot materials needed business end trip driving breeze everything worked right complaints standard making car except comfort area important comfort long trip uncomfortable driver distracting made trip seem unnecessarily longer felt make trip lot saturn would better choice rented model quite often offers comforts taurus simply planning long trip buy car intend comfortably seated check taurus longer trial basis might another car may find thing happening comfort \n",
      "\n",
      "----------------------\n",
      "\n",
      "Original: \n",
      " This is a sample review! It's great, isn't it? \n",
      "\n",
      "Processed: \n",
      " sample review great \n",
      "\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def words_and_punctuation_removal_example():\n",
    "    \"\"\"\n",
    "    Example of stopword removal, punctuation removal, and lowercasing, as part of the marking criteria.\n",
    "    \"\"\"\n",
    "\n",
    "    # 2 example rows from car-reviews.csv\n",
    "    example_reviews = input_data[\"Review\"].head(2).tolist()\n",
    "\n",
    "    # additional made-up test review with punctuation\n",
    "    example_text = \"This is a sample review! It's great, isn't it?\"\n",
    "    example_reviews.append(example_text)\n",
    "\n",
    "    for text in example_reviews:\n",
    "        processed_review = preprocess_remove_punctuation(text)\n",
    "        processed_review = preprocess_lowercase(processed_review)\n",
    "        processed_review = preprocess_remove_stopwords(processed_review)\n",
    "\n",
    "        print(f\"Original: \\n {text} \\n\")\n",
    "        print(f\"Processed: \\n {processed_review} \\n\")\n",
    "        print(\"----------------------\\n\")\n",
    "\n",
    "words_and_punctuation_removal_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13c8e9-1482-4eeb-bdc3-aab9fa4941c3",
   "metadata": {},
   "source": [
    "The next cell demonstrates a call to preprocesser function in its entirety, which should also perform lemmatisation on words such that words with the same stem are recognised and subequent tokens produced are that of the root stem.\n",
    "\n",
    "As previously mentioned it is this prepocesser function that is used in a transform step as part of the sklearn Pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60fe74f0-bd98-4e3d-8dfd-14877cc0f06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      " Here's some words with the same root stems! Running, run, runs. \n",
      "\n",
      "Processed: \n",
      " word root stem run run run \n",
      "\n",
      "----------------------\n",
      "\n",
      "Original: \n",
      " ... And some more! Removed, remove, removing. \n",
      "\n",
      "Processed: \n",
      " remove remove remove \n",
      "\n",
      "----------------------\n",
      "\n",
      "Original: \n",
      " ... And more! Driving, Drove, Drive. \n",
      "\n",
      "Processed: \n",
      " drive drive drive \n",
      "\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocessor_example():\n",
    "    \"\"\"Example of preprocessor function usage.\"\"\"\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    # additional made-up test review with punctuation\n",
    "    stem_example_1 = \"Here's some words with the same root stems! Running, run, runs.\"\n",
    "    stem_example_2 = \"... And some more! Removed, remove, removing.\"\n",
    "    stem_example_3 = \"... And more! Driving, Drove, Drive.\"\n",
    "\n",
    "    examples.append(stem_example_1)\n",
    "    examples.append(stem_example_2)\n",
    "    examples.append(stem_example_3)\n",
    "\n",
    "    for text in examples:\n",
    "        processed_review = preprocessor(text)\n",
    "        print(f\"Original: \\n {text} \\n\")\n",
    "        print(f\"Processed: \\n {processed_review} \\n\")\n",
    "        print(\"----------------------\\n\")\n",
    "\n",
    "preprocessor_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a192af-9e2f-4c48-9b47-43bf7ec2aeb5",
   "metadata": {},
   "source": [
    "The next cell below runs a function that demonstrates that the ML pipeline implemented uses the CountVectorizer which uses a bag-of-words based approach, that is, for all of the input data an overall vocabulary is maintained for all word stems in all reviews, and that for each single review a vector is produced with word stem occurence counts.\n",
    "\n",
    "The function includes some additional steps \"DebugStep\" to enable investigation of the data after each step of the pipeline, of most interest is the comparison of the data after the preprocessing transform and the vectorization transform step. Such a comparison is printed at the bottom of the function which demonstrates that the vectorization works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2488ce3f-dc73-44a1-928e-b6acd1fadfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DebugStep: Transforming data...\n",
      "DebugStep: Transforming data...\n",
      "Vectorizer Output Shape:\n",
      "(1106, 12185)\n",
      "Vocabulary Length:\n",
      "12185\n",
      "###### Review 0 vector ######\n",
      "\n",
      "Processed Text:\n",
      "\n",
      "shop new vehicle want something would get back forth work something fun drive weekend think get far take 97 explorer sport four wheeling place people take beat roaders take trail wood swamp snow drift do play take car wash hose put new coat wax ready go work monday like vehicle minor problem one bad sensor 4 wheel drive selenoid truck would shift 4 low come would shut start fixed warranty occoured since usual maintenance item need replace truck would recommend anyone look functional fun vehicle rid handle great road\n",
      "\n",
      "Vector:\n",
      "\n",
      "   shop  new  vehicle  want  something  would  get  back  forth  work  ...  \\\n",
      "0     1    2        3     1          2      4    2     1      1     2  ...   \n",
      "\n",
      "   hogan  joes  labeling  bilstein  manufactured  autolocking  265x15  55k  \\\n",
      "0      0     0         0         0             0            0       0    0   \n",
      "\n",
      "   crutchfield  tis  \n",
      "0            0    0  \n",
      "\n",
      "[1 rows x 12185 columns]\n",
      "\n",
      "----------------------\n",
      "\n",
      "###### Review 1 vector ######\n",
      "\n",
      "Processed Text:\n",
      "\n",
      "even know begin car many problem seem like everyday another problem first fair list good part car 1 6 liter h engine produce lot power small engine actually beat v8 chevys ford line cruise problem high speed thats probably good thing car car bland bore design actually look pretty ugly ask get interior cramp back seat problem 1 leak winshield sunroof 2 car loud cant hear think 3 turn vent get smell gasoline 4 interior smell horrible 5 handle stink power steer squeak turn 6 shock suspension horrible feel every bump road 7 horrible gas mileage 15 19 mpg think suppose economy car 8 frequent problem engine overheat burn leaking oil 9 car spent time mechanic garage mine 10 half option car fail cruise control ac etc ok pretty much message stay away car worth 1000 treat vacation something definitely need one deal car\n",
      "\n",
      "Vector:\n",
      "\n",
      "   shop  new  vehicle  want  something  would  get  back  forth  work  ...  \\\n",
      "0     0    0        0     0          1      0    2     1      0     0  ...   \n",
      "\n",
      "   hogan  joes  labeling  bilstein  manufactured  autolocking  265x15  55k  \\\n",
      "0      0     0         0         0             0            0       0    0   \n",
      "\n",
      "   crutchfield  tis  \n",
      "0            0    0  \n",
      "\n",
      "[1 rows x 12185 columns]\n",
      "\n",
      "----------------------\n",
      "\n",
      "###### Review 2 vector ######\n",
      "\n",
      "Processed Text:\n",
      "\n",
      "bought 1999 windstar december 2000 26 000 mile bargiend dealer get 6 year 120k warrenty first impression like deep red like good wine hence name dark cabernet red interior nicley appoint look like do cheap tire also cheap blame dealer replacement good radio good c comfertably fit family 6 drivetrain 3 8 v 6 plenty get go get 17 around towan 23 interstate transsmisson usually shift smothly handle fine become bit hard handle emergancy situation fairly easy control dealer put ameri general tire replace good set dunlops brake ok interior cheap several small plastic bit fell rear c control fell hard brake rear c work seat comfertable road trip family never complain cd cassette deliver pleanty good rich sound reliablity complaint longer van ford make good trannies one exception bad normal whent around 60k fortunatly extend warrenty pick plus numorus electrical issue interior fall apart build usual windstar problem little reserch like car tracker see mean reccomend one\n",
      "\n",
      "Vector:\n",
      "\n",
      "   shop  new  vehicle  want  something  would  get  back  forth  work  ...  \\\n",
      "0     0    0        0     0          0      0    3     0      0     1  ...   \n",
      "\n",
      "   hogan  joes  labeling  bilstein  manufactured  autolocking  265x15  55k  \\\n",
      "0      0     0         0         0             0            0       0    0   \n",
      "\n",
      "   crutchfield  tis  \n",
      "0            0    0  \n",
      "\n",
      "[1 rows x 12185 columns]\n",
      "\n",
      "----------------------\n",
      "\n",
      "###### Review 3 vector ######\n",
      "\n",
      "Processed Text:\n",
      "\n",
      "recently receive new 2002 ford windstar company vehicle say extremely bad choice first body style boring compare rest mini van market nose long body box accent line spruce interior equally bore except maybe cd player rear seat bench style bucket style chevy venture make difficult get even small person remove seat take whole bench instead simply one seat headache weight doubt mother three could attempt feat space rear behind last bench seat non existent sure even exist actually eliminate space back seat little bit leg room push engine loud sound like go explode suck 20 gallon gas tank matter 350 mile ride bad stereotype drive boat full force transmission little choppy new vehicle cant imagine like 50 000 mile ford never know transmission first place steer column seem little center relation driver seat due dash board design pro see applies free vehicle unfortunately 29 grand beauty favor shop around guarantee find good selection different dealer\n",
      "\n",
      "Vector:\n",
      "\n",
      "   shop  new  vehicle  want  something  would  get  back  forth  work  ...  \\\n",
      "0     1    2        3     0          0      0    1     1      0     0  ...   \n",
      "\n",
      "   hogan  joes  labeling  bilstein  manufactured  autolocking  265x15  55k  \\\n",
      "0      0     0         0         0             0            0       0    0   \n",
      "\n",
      "   crutchfield  tis  \n",
      "0            0    0  \n",
      "\n",
      "[1 rows x 12185 columns]\n",
      "\n",
      "----------------------\n",
      "\n",
      "###### Review 4 vector ######\n",
      "\n",
      "Processed Text:\n",
      "\n",
      "taurus great car buy 1996 low mileage everything work fine extremely comfortable 1999 come boom 100 000 kilometer air conditioner quit 120 000 kilometer head gasket blow 155 000 transmission completely rebuilt 1994 taurus gl equip 3 8 engine learn never buy product ford 3 8 engine car go scrap yard 185 000 kilometer car expensive lesson learn ford rebate money head gasket repair believe happenned due faulty ford 3 8 engine whatever happenned car last 10 year keep mind read article use kilometer mile 100 000 kilometer 60 000 mile\n",
      "\n",
      "Vector:\n",
      "\n",
      "   shop  new  vehicle  want  something  would  get  back  forth  work  ...  \\\n",
      "0     0    0        0     0          0      0    0     0      0     1  ...   \n",
      "\n",
      "   hogan  joes  labeling  bilstein  manufactured  autolocking  265x15  55k  \\\n",
      "0      0     0         0         0             0            0       0    0   \n",
      "\n",
      "   crutchfield  tis  \n",
      "0            0    0  \n",
      "\n",
      "[1 rows x 12185 columns]\n",
      "\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words_vector_example():\n",
    "    \"\"\"\n",
    "    Example of bag-of-words based vectorization using the CountVectorizer from sklearn.\n",
    "    \"\"\"\n",
    "\n",
    "    class DebugStep(BaseEstimator, TransformerMixin):\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            print(\"DebugStep: Transforming data...\")\n",
    "            self.previous_data = X\n",
    "            return X\n",
    "\n",
    "    test_size = 276  # 20% of the data for testing\n",
    "\n",
    "    training_data, test_data = train_test_split(\n",
    "        input_data,\n",
    "        test_size=test_size,\n",
    "        random_state=120,\n",
    "        stratify=input_data[\"Sentiment\"],\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"preprocess\", Preprocessor()),\n",
    "            (\"preprocess_debug\", DebugStep()),\n",
    "            (\"vectorize\", CountVectorizer()),\n",
    "            (\"vectorize_debug\", DebugStep()),\n",
    "            (\"classify\", MultinomialNB()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline.fit(training_data[\"Review\"], training_data[\"Sentiment\"])\n",
    "\n",
    "    # analyse preprocessor\n",
    "    preprocessor = pipeline.named_steps[\"preprocess\"]\n",
    "    preprocessor_output = pipeline.named_steps[\"preprocess_debug\"].previous_data\n",
    "\n",
    "    # anaylse CountVectorizer\n",
    "    vectorizer = pipeline.named_steps[\"vectorize\"]\n",
    "    vocabulary = vectorizer.vocabulary_\n",
    "    index_to_word = {index: word for word, index in vocabulary.items()}\n",
    "\n",
    "    # get output of CountVectorizer\n",
    "    vectorizer_output = pipeline.named_steps[\"vectorize_debug\"].previous_data\n",
    "\n",
    "    print(\"Vectorizer Output Shape:\")\n",
    "    print(vectorizer_output.shape)\n",
    "\n",
    "    print(\"Vocabulary Length:\")\n",
    "    print(len(vocabulary))\n",
    "\n",
    "    # first 5 review vectors\n",
    "    for i in range(5):\n",
    "        indices = vectorizer_output[i].indices\n",
    "        vector_for_row = vectorizer_output.A[i]\n",
    "\n",
    "        word_to_count = {word: 0 for word in index_to_word.values()}\n",
    "\n",
    "        for indicy in indices:\n",
    "            word_to_count[index_to_word[indicy]] = vector_for_row[indicy]\n",
    "\n",
    "        # add to data frame for nice printing\n",
    "        vector_to_vocabulary = pd.DataFrame(word_to_count, index=[0])\n",
    "\n",
    "        print(f\"###### Review {i} vector ######\\n\")\n",
    "        print(\"Processed Text:\\n\")\n",
    "        print(preprocessor_output[i])\n",
    "        print(\"\\nVector:\\n\")\n",
    "        print(vector_to_vocabulary)\n",
    "        print(\"\\n----------------------\\n\")\n",
    "\n",
    "bag_of_words_vector_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b6756-93a7-4f03-b90a-b2df54e63ce5",
   "metadata": {},
   "source": [
    "The final code cell for part 1 below demonstrates the entire pipeline in action with the performance metrics printed in a confusion matrix.\n",
    "\n",
    "This code in the run_ml_pipeline function shows that 80% of the data is used to train the model and that 20% is used as the test data.\n",
    "\n",
    "Again - Please see the run_ml_pipeline code for reasoning behind the mechanism that handles unseen words in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86a29cb9-55ce-4ecc-aee4-809fb3a15caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.81%\n",
      "Confusion Matrix Proportions:\n",
      "True Negative: 39.13%\n",
      "False Negative: 12.32%\n",
      "True Positive: 37.68%\n",
      "False Positive: 10.87%\n"
     ]
    }
   ],
   "source": [
    "run_ml_pipeline(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c1825-7b85-4adb-b674-d5f6e60ed5be",
   "metadata": {},
   "source": [
    "# Task 2 - Improved Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668335c4-6294-4294-ad78-0a5b315d1220",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This task aims to understand the potential pitfalls of the Naive Bayes implementation and what alternatives may perform better, we will consider the estimator and understand ways to improve on other parts of the Machine Learning (ML) pipeline, such as feature extraction, hyperparameter tuning and model evaluation.\n",
    "\n",
    "# Model Investigation\n",
    "\n",
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes works on the assumption that every predictor in a model is conditionally-independent, the benefit of this is that by using this assumption, we can get reasonably accurate estimations for very little compute power as such Naive Bayes lends itself well to classification tasks where data quantity is limited and can be done in real-time applications. (citation needed)\n",
    "\n",
    "This simplifying assumption is what credits the \"Naive\" label; assuming that every feature is conditionally-independent doesn't model real-world data. For instance, when discussing sentiment analysis, word order and context convey a critically different meaning to their individual parts. One word could convey a completely different sentiment depending on their neighbouring words or position in a sentence. Therefore, Naive Bayes should not be the best fit to classify sentiment, as we potentially lose information. (citation needed)\n",
    "\n",
    "## Alternatives\n",
    "\n",
    "An alternative classifier must improve on the downsides demonstrated by Naive Bayes in the previous chapter while also being a generally good for sentiment analysis in the context of this assignments specific problem. Sentiment analysis is a problem that produces high dimensional data, that is data with many features, so an alternative must be good at handling high dimensional data. The alternative must also be able to capture the complex meaning of words depending on their context, this could be analogous to capturing non-linear relationships (citation needed).\n",
    "\n",
    "### Support Vector Machines (SVM)\n",
    "\n",
    "SVMs are known to be a good fit for classification tasks (citation needed). They are typically considered a linear classifier but tricks can be employed using the right Kernel to introduce non-linear classification boundaries. They also handle high dimensional data well (citation needed).\n",
    "\n",
    "### Recurrent Neural Networks (RNN)\n",
    "\n",
    "RNNs are designed to handle sequential data, such as textual reviews. Unlike a typical Feed Forward Neural Network (FNN), RNNs use recurrent connections, this works by remembering the previous state and feeding it into the same neuron in the next temporal step. This ability to recur information temporary allows RNNs to capture complex patterns and dependencies between features, which is very useful for the purpose of sentiment analysis where information is embededd within a words context with other words around it (citation needed).\n",
    "\n",
    "We can summarise the benefits as the ability to understand complex linguistic nuance and high accuracy and reliabilty, the downsides of RNN include high complexity, long training times and require more compute than previous traditional ML alternatives (Mao, Liu and Zhang, 2024).\n",
    "\n",
    "### Transformer\n",
    "\n",
    "Transformer architecture is now typically the go-to for sequential data related tasks (citation needed). The downside of implementing a transformer based architecture is they require a hugh amount of data (Mao, Liu and Zhang, 2024).\n",
    "\n",
    "\n",
    "# Approach\n",
    "\n",
    "I have selected to build an RNN to hopefully improve the performance over Naive Bayes, due to its inherent ability to remember information and then infer  \n",
    "\n",
    "- Model: RNN\n",
    "- Tooling: pytorch\n",
    "- Feature extraction: Word Embedding (FastText / Word2Vec)\n",
    "- Architecture: LTSM\n",
    "\n",
    "# Results\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\n",
    "# References\n",
    "\n",
    "Mao, Y., Liu, Q. and Zhang, Y., 2024. Sentiment analysis methods, applications, and challenges: A systematic literature review. Journal of King Saud University - Computer and Information Sciences [Online], 36(4), p.102048. Available from: https://doi.org/10.1016/j.jksuci.2024.102048.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed261a-6905-4586-86c7-d43f9014b7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
